{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question! Let's clarify how PCA-transformed data corresponds to your **target values (`y`)** during training, testing, and validation.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **How PCA Works with Target Labels (`y`)**\n",
    "- PCA **only transforms the features (`X`)** and does **not affect the target (`y`)**.\n",
    "- Your target labels (like `Dataset_Type_benign`) **remain unchanged** because PCA is an **unsupervised method** that doesn't consider target labels.\n",
    "- However, the order of the rows is preserved. This means that the **PCA-transformed features still correspond to the correct `y` labels**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **How to Use PCA for Training, Testing, and Validation**\n",
    "\n",
    "### 1. **Fit PCA on the Training Set Only**\n",
    "- To **avoid data leakage**, always:\n",
    "   - **Fit** the PCA on the **training set**.\n",
    "   - **Transform** the training, validation, and test sets using the **same fitted PCA**.\n",
    "\n",
    "### 2. **Split the Data** (Before Applying PCA)\n",
    "- First, split the data into training, validation, and test sets.\n",
    "\n",
    "### 3. **Fit PCA and Transform**:\n",
    "- Fit the PCA only on the **training features (`X_train`)**.\n",
    "- Then, **transform** `X_train`, `X_val`, and `X_test` using this fitted PCA object.\n",
    "\n",
    "### 4. **Keep Target Labels (`y`) Unchanged**:\n",
    "- The corresponding `y` values stay the same because the row order is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• **Code Example: Training with PCA**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "# -------------------- STEP 1: Split the Data --------------------\n",
    "X = final_df.drop(columns=['Dataset_Type_benign', 'Dataset_Type_ccRCC', 'Dataset_Type_nccRCC'])\n",
    "y = final_df[['Dataset_Type_benign', 'Dataset_Type_ccRCC', 'Dataset_Type_nccRCC']]\n",
    "\n",
    "# Train-Test-Validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# -------------------- STEP 2: Scale the Features --------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------- STEP 3: Fit PCA on Training Set --------------------\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# -------------------- STEP 4: Convert to PyTorch Tensors --------------------\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_pca, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# -------------------- STEP 5: Prepare PyTorch DataLoader --------------------\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training Set Shape: {X_train_tensor.shape}, Labels Shape: {y_train_tensor.shape}\")\n",
    "print(f\"Validation Set Shape: {X_val_tensor.shape}, Labels Shape: {y_val_tensor.shape}\")\n",
    "print(f\"Test Set Shape: {X_test_tensor.shape}, Labels Shape: {y_test_tensor.shape}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Why is This Approach Correct?**\n",
    "\n",
    "1. **No Data Leakage**:\n",
    "   - PCA is only fitted on the **training data**.\n",
    "   - Validation and test sets are **only transformed**, ensuring no leakage.\n",
    "\n",
    "2. **Target Correspondence**:\n",
    "   - Since the PCA transformation **preserves the row order**, the corresponding `y` values (like `\"benign\"` or `\"ccRCC\"`) stay correctly aligned.\n",
    "\n",
    "3. **Efficient Training**:\n",
    "   - The reduced feature set from PCA ensures a **lower-dimensional and more efficient dataset** for faster model training.\n",
    "\n",
    "4. **Scalability**:\n",
    "   - Works seamlessly with PyTorch‚Äôs `DataLoader` for batch training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **What Happens If I Don't Split First?**\n",
    "- If you fit PCA on the **entire dataset** and then split, **data leakage** will occur.\n",
    "   - The model could learn patterns from the validation/test set that it shouldn‚Äôt know about during training.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Final Summary**\n",
    "- PCA-transformed features remain aligned with their original `y` values because **PCA preserves row order**.\n",
    "- Always **fit PCA on the training set only** and **transform other datasets**.\n",
    "- This ensures accurate and leakage-free model training.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to proceed with this approach for your model training, or explore any other steps in more detail? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example code\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "# -------------------- STEP 1: Split the Data --------------------\n",
    "X = final_df.drop(columns=['Dataset_Type_benign', 'Dataset_Type_ccRCC', 'Dataset_Type_nccRCC'])\n",
    "y = final_df[['Dataset_Type_benign', 'Dataset_Type_ccRCC', 'Dataset_Type_nccRCC']]\n",
    "\n",
    "# Train-Test-Validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# -------------------- STEP 2: Scale the Features --------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------- STEP 3: Fit PCA on Training Set --------------------\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# -------------------- STEP 4: Convert to PyTorch Tensors --------------------\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_pca, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# -------------------- STEP 5: Prepare PyTorch DataLoader --------------------\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training Set Shape: {X_train_tensor.shape}, Labels Shape: {y_train_tensor.shape}\")\n",
    "print(f\"Validation Set Shape: {X_val_tensor.shape}, Labels Shape: {y_val_tensor.shape}\")\n",
    "print(f\"Test Set Shape: {X_test_tensor.shape}, Labels Shape: {y_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö **Understanding Variance in PCA**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **What is Variance?**\n",
    "- **Variance** measures how **spread out** the data is in relation to its mean.\n",
    "- Mathematically, it is the **average of the squared differences** from the mean:\n",
    "\n",
    "\\[\n",
    "\\text{Variance} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "\\]\n",
    "\n",
    "- Where:\n",
    "  - \\(x_i\\) is a data point.\n",
    "  - \\(\\mu\\) is the mean of the dataset.\n",
    "  - \\(n\\) is the total number of data points.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **What Does Variance Represent in PCA?**\n",
    "\n",
    "- In PCA, **variance** indicates how much information or spread is captured by each **principal component (PC)**.\n",
    "- The **greater the variance** captured by a component, the more **important** that component is for representing the data.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **How Total Variance is Calculated in PCA**\n",
    "\n",
    "1. **Start with the Covariance Matrix**:\n",
    "   - PCA is based on the **covariance matrix** of the dataset, which shows how features vary with each other.\n",
    "\n",
    "2. **Eigenvalues and Eigenvectors**:\n",
    "   - The **eigenvalues** of the covariance matrix represent the **amount of variance** captured by their corresponding eigenvectors (which become the principal components).\n",
    "   - The **sum of all eigenvalues** gives the **total variance** in the dataset.\n",
    "\n",
    "\\[\n",
    "\\text{Total Variance} = \\sum_{i=1}^{k} \\lambda_i\n",
    "\\]\n",
    "\n",
    "- Where \\( \\lambda_i \\) is the eigenvalue for the \\(i\\)-th principal component.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **How is Explained Variance Ratio Calculated?**\n",
    "\n",
    "- The **explained variance ratio** for each principal component is calculated as:\n",
    "\n",
    "\\[\n",
    "\\text{Explained Variance Ratio for PC}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{k} \\lambda_j}\n",
    "\\]\n",
    "\n",
    "- Where:\n",
    "  - \\( \\lambda_i \\) is the eigenvalue for the \\(i\\)-th principal component.\n",
    "  - The denominator is the **total variance**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **What Does Explained Variance Ratio Tell Us?**\n",
    "- **Higher Ratio**: The component captures more information (variance) from the data.\n",
    "- **Lower Ratio**: The component captures less variation.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Why Does PCA Focus on Variance?**\n",
    "- PCA identifies directions (**principal components**) where the **variance is maximized**.\n",
    "- This is because **high variance** suggests **important and diverse information**, while low variance often indicates **noise**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Example: Variance Calculation in PCA**\n",
    "\n",
    "| **Component** | **Eigenvalue (\\(\\lambda\\))** | **Explained Variance Ratio** |\n",
    "|---------------|-----------------------------|------------------------------|\n",
    "| PC1           | 4.5                         | \\( \\frac{4.5}{10} = 45\\%\\)   |\n",
    "| PC2           | 3.0                         | \\( \\frac{3.0}{10} = 30\\%\\)   |\n",
    "| PC3           | 2.0                         | \\( \\frac{2.0}{10} = 20\\%\\)   |\n",
    "| PC4           | 0.5                         | \\( \\frac{0.5}{10} = 5\\%\\)    |\n",
    "| **Total**     | **10.0**                    | **100%**                     |\n",
    "\n",
    "- In this example:\n",
    "  - The **total variance** is `10` (sum of eigenvalues).\n",
    "  - PC1 explains **45%** of the variance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Key Takeaways**\n",
    "1. **Variance in PCA** refers to how much spread or information a principal component captures.\n",
    "2. The **total variance** is the **sum of all eigenvalues**.\n",
    "3. The **explained variance ratio** tells you how much of the dataset's information is retained by each component.\n",
    "4. PCA reduces dimensions by selecting the components that retain the **most variance** (information).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show how your dataset's total variance and explained variance ratio were calculated, step by step? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer for chosing the 95% of variance in PCA**\n",
    "\n",
    "The reason why I'm choosing a threshold of 95% for my PCA is to retain enough data variance for accuracy while making the process efficient. This value is set only for investigation purposes.\n",
    "\n",
    "As a result, 95% of the variance contains 56 principal components, which is acceptable. Too many components may indicate noisy features and additional processes to reduce the irrelevant features\n",
    "\n",
    "Moving to the next step to train our model, we could tune the threshold of variance as a hyperparameter. The ideal threshold depends on the trade-off between model accuracy and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
